{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a44a6e",
   "metadata": {},
   "source": [
    "# Value-Difference Based Q-Learning\n",
    "\n",
    "Preston Whitcomb & Evan Schubert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b705a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from agent import Agent\n",
    "from VDBE_agent import VDBE_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bdf57",
   "metadata": {},
   "source": [
    "### Graphing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_table_magnitude(q_values : dict):\n",
    "    # calculate L1 norm of q_value matrix \n",
    "    # each key is a column, each value of the key is a row element\n",
    "    # we take the norm of all columns, \n",
    "    # then find the max for the overall matrix norm\n",
    "    norms = []\n",
    "    for key in q_values.keys():\n",
    "        n = np.linalg.norm(list((q_values[key].values())))\n",
    "        norms.append(n)\n",
    "    norm_max = max(norms)\n",
    "    return norm_max\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def graph_x_over_time(x : list, line_label : str, x_label : str, y_label : str, title : str, fig=None, axs=None):\n",
    "    if fig is None or axs is None:\n",
    "        fig, axs = plt.subplots() # allows adding on to existing graph\n",
    "    # graph the data\n",
    "    axs.plot(x, label=line_label)\n",
    "    # add or update the labels\n",
    "    axs.set_xlabel(x_label)\n",
    "    axs.set_xlabel(y_label)\n",
    "    axs.set_title(title)\n",
    "    axs.legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354da4c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent : Agent, env_name: str, num_episodes=10_000, render=False):\n",
    "    env = gymnasium.make(env_name, render_mode=\"human\" if render else None)\n",
    "\n",
    "    magnitudes_over_time = []\n",
    "    rewards_over_time = []\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        # Reset environment to start a new episode\n",
    "        observation, info = env.reset()\n",
    "        observation = tuple(observation) if type(observation) == np.ndarray else observation\n",
    "        episode_over = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not episode_over:\n",
    "            # Choose an action\n",
    "            action = agent.pi(observation)\n",
    "\n",
    "            # Take the action and see what happens\n",
    "            new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            new_observation = tuple(new_observation) if type(new_observation) == np.ndarray else new_observation\n",
    "\n",
    "            # reward: +1 for each step the pole stays upright\n",
    "            # terminated: True if pole falls too far (agent failed)\n",
    "            # truncated: True if we hit the time limit (500 steps)\n",
    "\n",
    "            total_reward += reward\n",
    "            episode_over = terminated or truncated\n",
    "\n",
    "            agent.update_Q_learning(new_observation, action, reward, observation, episode_over)\n",
    "\n",
    "            observation = new_observation\n",
    "\n",
    "        # store episode results\n",
    "        q_magnitude = calc_q_table_magnitude(agent.Q)\n",
    "        magnitudes_over_time.append(q_magnitude)\n",
    "        rewards_over_time.append(total_reward)\n",
    "        \n",
    "    env.close()\n",
    "    return agent, magnitudes_over_time, rewards_over_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409965d4",
   "metadata": {},
   "source": [
    "# Making Agents, Env, and Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b80c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "environments_dict= {\"LunarLander-v3\": [0, 1, 2, 3], \"Blackjack-v1\": [0,1], \"CliffWalking-v1\": [0, 1, 2, 3],\"Taxi-v3\": [0, 1, 2, 3, 4, 5],}\n",
    "\n",
    "\n",
    "num_turns_to_train = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457d6b6",
   "metadata": {},
   "source": [
    "# Train Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_histories_per_environment = {env : {agent : {} for agent in ['flat', 'decay', 'vdbe']} for env in environments_dict} # store {env {agent : {'magnitudes' : [], 'rewards' : []} } } \n",
    "for env_name in environments_dict:\n",
    "    # Make the agents with the appropriate action spaces\n",
    "    flat_epsilon_agent = Agent(environments_dict[env_name])\n",
    "    epsilon_decay_agent = Agent(environments_dict[env_name], epsilon=1.0, do_epsilon_decay=True)\n",
    "    vdbe_agent = VDBE_agent(environments_dict[env_name])\n",
    "    # Train\n",
    "    print(f\"Training agents on {env_name}...\")\n",
    "    trained_flat_epsilon_agent, flat_mag, flat_reward = train(flat_epsilon_agent, env_name, num_episodes=num_turns_to_train, render=False)\n",
    "    trained_epsilon_decay_agent, decay_mag, decay_reward = train(epsilon_decay_agent, env_name, num_episodes=num_turns_to_train, render=False)\n",
    "    trained_vdbe_agent, vdbe_mag, vdbe_reward = train(vdbe_agent, env_name, num_episodes=num_turns_to_train, render=False)\n",
    "    # Store results for the current environment\n",
    "    training_histories_per_environment[env_name]['flat']['magnitudes'] = flat_mag\n",
    "    training_histories_per_environment[env_name]['flat']['rewards'] = flat_reward\n",
    "    training_histories_per_environment[env_name]['decay']['magnitudes'] = decay_mag\n",
    "    training_histories_per_environment[env_name]['decay']['rewards'] = decay_reward\n",
    "    training_histories_per_environment[env_name]['vdbe']['magnitudes'] = vdbe_mag\n",
    "    training_histories_per_environment[env_name]['vdbe']['rewards'] = vdbe_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a21c48",
   "metadata": {},
   "source": [
    "# Graph magnitudes and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928aae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_step_size = num_turns_to_train / 100\n",
    "for metric in ['magnitudes', 'rewards']:\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(20, 4))\n",
    "    for col, env in enumerate(training_histories_per_environment):\n",
    "        # plot all magnitudes in a graph\n",
    "        for agent in training_histories_per_environment[env]:\n",
    "            data = training_histories_per_environment[env][agent][metric]\n",
    "            # axs[col].plot(data, label=agent)\n",
    "            axs[col].plot(moving_average(data, len(data) // moving_average_step_size), label=agent)\n",
    "        axs[col].set_xlabel(\"Episode\")\n",
    "        axs[col].set_ylabel(metric.title())\n",
    "        axs[col].set_title(f\"{env.title()} {metric.title()}\")\n",
    "        axs[col].legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
